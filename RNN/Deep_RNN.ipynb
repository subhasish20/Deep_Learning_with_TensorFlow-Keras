{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1622,
      "metadata": {
        "id": "ZCUWTBppfCwY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1623,
      "metadata": {
        "id": "y6OQowK3fTjV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1624,
      "metadata": {
        "id": "o6eOt-BLfXBk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN,Dense,Embedding,Bidirectional\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1625,
      "metadata": {
        "id": "_ESQyhj_fdQ1"
      },
      "outputs": [],
      "source": [
        "data = \"\"\"Good morning! How did you sleep last night? Did you have any interesting dreams?\n",
        "Hey there! I was just thinking about you the other day. How have you been doing?\n",
        "What a beautiful day outside! Have you had a chance to enjoy the weather yet?\n",
        "I'm so glad we finally have some time to catch up. What's new in your life?\n",
        "Could you believe how crowded the supermarket was this morning? It was insane!\n",
        "I just wanted to say thank you again for helping me move last weekend. You're amazing!\n",
        "How was your weekend? Did you end up going to that concert you mentioned?\n",
        "I'm really sorry I'm running late. Traffic is absolutely terrible this morning.\n",
        "What do you feel like having for lunch today? I'm craving something spicy myself.\n",
        "Have you seen the new coffee shop that opened down the street? It looks really nice.\n",
        "I can't believe how fast this week has gone by. Is it Friday already?\n",
        "Would you like to grab dinner sometime this week? I know a great new restaurant.\n",
        "How's your family doing? I heard your sister just had a baby. Congratulations!\n",
        "I'm so tired today. I stayed up way too late watching that new show on Netflix.\n",
        "Did you hear about the big sale happening at the mall this weekend? We should go!\n",
        "What's your plan for the holidays? Are you traveling anywhere exciting this year?\n",
        "I love your outfit today! That color really brings out your eyes beautifully.\n",
        "Could you help me with this computer problem? I'm completely stuck right now.\n",
        "How's work been treating you lately? You seem really busy these past few weeks.\n",
        "I'm thinking about redecorating my living room. What do you think would look nice?\n",
        "The weather forecast says it might rain tomorrow. Don't forget your umbrella!\n",
        "I just finished reading the most amazing book. You should definitely check it out.\n",
        "Have you tried that new Italian restaurant on Main Street? The pasta is incredible!\n",
        "I'm so excited for summer vacation. We're planning to visit Europe this year!\n",
        "How are you feeling today? You look much better than the last time I saw you.\n",
        "I wanted to apologize for missing your birthday party. Something came up at work.\n",
        "What's your favorite thing to do on a lazy Sunday afternoon? I love reading.\n",
        "Could you recommend a good mechanic? My car has been making strange noises.\n",
        "I'm really looking forward to the weekend. This work week has been exhausting!\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1626,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzvVG5nKf1RO",
        "outputId": "2dd46a6b-6a04-441c-9c9f-06a1e026a195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good morning! How did you sleep last night? Did you have any interesting dreams?\n",
            "Hey there! I was just thinking about you the other day. How have you been doing?\n",
            "What a beautiful day outside! Have you had a chance to enjoy the weather yet?\n",
            "I'm so glad we finally have some time to catch up. What's new in your life?\n",
            "Could you believe how crowded the supermarket was this morning? It was insane!\n",
            "I just wanted to say thank you again for helping me move last weekend. You're amazing!\n",
            "How was your weekend? Did you end up going to that concert you mentioned?\n",
            "I'm really sorry I'm running late. Traffic is absolutely terrible this morning.\n",
            "What do you feel like having for lunch today? I'm craving something spicy myself.\n",
            "Have you seen the new coffee shop that opened down the street? It looks really nice.\n",
            "I can't believe how fast this week has gone by. Is it Friday already?\n",
            "Would you like to grab dinner sometime this week? I know a great new restaurant.\n",
            "How's your family doing? I heard your sister just had a baby. Congratulations!\n",
            "I'm so tired today. I stayed up way too late watching that new show on Netflix.\n",
            "Did you hear about the big sale happening at the mall this weekend? We should go!\n",
            "What's your plan for the holidays? Are you traveling anywhere exciting this year?\n",
            "I love your outfit today! That color really brings out your eyes beautifully.\n",
            "Could you help me with this computer problem? I'm completely stuck right now.\n",
            "How's work been treating you lately? You seem really busy these past few weeks.\n",
            "I'm thinking about redecorating my living room. What do you think would look nice?\n",
            "The weather forecast says it might rain tomorrow. Don't forget your umbrella!\n",
            "I just finished reading the most amazing book. You should definitely check it out.\n",
            "Have you tried that new Italian restaurant on Main Street? The pasta is incredible!\n",
            "I'm so excited for summer vacation. We're planning to visit Europe this year!\n",
            "How are you feeling today? You look much better than the last time I saw you.\n",
            "I wanted to apologize for missing your birthday party. Something came up at work.\n",
            "What's your favorite thing to do on a lazy Sunday afternoon? I love reading.\n",
            "Could you recommend a good mechanic? My car has been making strange noises.\n",
            "I'm really looking forward to the weekend. This work week has been exhausting!\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1627,
      "metadata": {
        "id": "7qvmCUaQf2FW"
      },
      "outputs": [],
      "source": [
        "# creating an object of tokenizer class\n",
        "tokenizer = Tokenizer(oov_token='<NOTHING>') # if any word came out side the train data it will simple print nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1628,
      "metadata": {
        "id": "qyeooTX6f6sG"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts([data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1629,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TcQK_aBf_z-",
        "outputId": "05db5408-933e-4c13-a821-ab630459cc65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<NOTHING>': 1,\n",
              " 'you': 2,\n",
              " 'the': 3,\n",
              " 'i': 4,\n",
              " 'your': 5,\n",
              " 'to': 6,\n",
              " \"i'm\": 7,\n",
              " 'this': 8,\n",
              " 'how': 9,\n",
              " 'have': 10,\n",
              " 'a': 11,\n",
              " 'new': 12,\n",
              " 'it': 13,\n",
              " 'for': 14,\n",
              " 'that': 15,\n",
              " 'really': 16,\n",
              " 'did': 17,\n",
              " 'was': 18,\n",
              " 'just': 19,\n",
              " 'been': 20,\n",
              " 'up': 21,\n",
              " 'weekend': 22,\n",
              " 'today': 23,\n",
              " 'morning': 24,\n",
              " 'last': 25,\n",
              " 'about': 26,\n",
              " 'what': 27,\n",
              " 'so': 28,\n",
              " \"what's\": 29,\n",
              " 'could': 30,\n",
              " 'is': 31,\n",
              " 'do': 32,\n",
              " 'week': 33,\n",
              " 'has': 34,\n",
              " 'on': 35,\n",
              " 'work': 36,\n",
              " 'good': 37,\n",
              " 'thinking': 38,\n",
              " 'day': 39,\n",
              " 'doing': 40,\n",
              " 'had': 41,\n",
              " 'weather': 42,\n",
              " 'we': 43,\n",
              " 'time': 44,\n",
              " 'believe': 45,\n",
              " 'wanted': 46,\n",
              " 'me': 47,\n",
              " 'amazing': 48,\n",
              " 'late': 49,\n",
              " 'like': 50,\n",
              " 'something': 51,\n",
              " 'street': 52,\n",
              " 'nice': 53,\n",
              " 'would': 54,\n",
              " 'restaurant': 55,\n",
              " \"how's\": 56,\n",
              " 'at': 57,\n",
              " 'should': 58,\n",
              " 'are': 59,\n",
              " 'year': 60,\n",
              " 'love': 61,\n",
              " 'out': 62,\n",
              " 'my': 63,\n",
              " 'look': 64,\n",
              " 'reading': 65,\n",
              " 'sleep': 66,\n",
              " 'night': 67,\n",
              " 'any': 68,\n",
              " 'interesting': 69,\n",
              " 'dreams': 70,\n",
              " 'hey': 71,\n",
              " 'there': 72,\n",
              " 'other': 73,\n",
              " 'beautiful': 74,\n",
              " 'outside': 75,\n",
              " 'chance': 76,\n",
              " 'enjoy': 77,\n",
              " 'yet': 78,\n",
              " 'glad': 79,\n",
              " 'finally': 80,\n",
              " 'some': 81,\n",
              " 'catch': 82,\n",
              " 'in': 83,\n",
              " 'life': 84,\n",
              " 'crowded': 85,\n",
              " 'supermarket': 86,\n",
              " 'insane': 87,\n",
              " 'say': 88,\n",
              " 'thank': 89,\n",
              " 'again': 90,\n",
              " 'helping': 91,\n",
              " 'move': 92,\n",
              " \"you're\": 93,\n",
              " 'end': 94,\n",
              " 'going': 95,\n",
              " 'concert': 96,\n",
              " 'mentioned': 97,\n",
              " 'sorry': 98,\n",
              " 'running': 99,\n",
              " 'traffic': 100,\n",
              " 'absolutely': 101,\n",
              " 'terrible': 102,\n",
              " 'feel': 103,\n",
              " 'having': 104,\n",
              " 'lunch': 105,\n",
              " 'craving': 106,\n",
              " 'spicy': 107,\n",
              " 'myself': 108,\n",
              " 'seen': 109,\n",
              " 'coffee': 110,\n",
              " 'shop': 111,\n",
              " 'opened': 112,\n",
              " 'down': 113,\n",
              " 'looks': 114,\n",
              " \"can't\": 115,\n",
              " 'fast': 116,\n",
              " 'gone': 117,\n",
              " 'by': 118,\n",
              " 'friday': 119,\n",
              " 'already': 120,\n",
              " 'grab': 121,\n",
              " 'dinner': 122,\n",
              " 'sometime': 123,\n",
              " 'know': 124,\n",
              " 'great': 125,\n",
              " 'family': 126,\n",
              " 'heard': 127,\n",
              " 'sister': 128,\n",
              " 'baby': 129,\n",
              " 'congratulations': 130,\n",
              " 'tired': 131,\n",
              " 'stayed': 132,\n",
              " 'way': 133,\n",
              " 'too': 134,\n",
              " 'watching': 135,\n",
              " 'show': 136,\n",
              " 'netflix': 137,\n",
              " 'hear': 138,\n",
              " 'big': 139,\n",
              " 'sale': 140,\n",
              " 'happening': 141,\n",
              " 'mall': 142,\n",
              " 'go': 143,\n",
              " 'plan': 144,\n",
              " 'holidays': 145,\n",
              " 'traveling': 146,\n",
              " 'anywhere': 147,\n",
              " 'exciting': 148,\n",
              " 'outfit': 149,\n",
              " 'color': 150,\n",
              " 'brings': 151,\n",
              " 'eyes': 152,\n",
              " 'beautifully': 153,\n",
              " 'help': 154,\n",
              " 'with': 155,\n",
              " 'computer': 156,\n",
              " 'problem': 157,\n",
              " 'completely': 158,\n",
              " 'stuck': 159,\n",
              " 'right': 160,\n",
              " 'now': 161,\n",
              " 'treating': 162,\n",
              " 'lately': 163,\n",
              " 'seem': 164,\n",
              " 'busy': 165,\n",
              " 'these': 166,\n",
              " 'past': 167,\n",
              " 'few': 168,\n",
              " 'weeks': 169,\n",
              " 'redecorating': 170,\n",
              " 'living': 171,\n",
              " 'room': 172,\n",
              " 'think': 173,\n",
              " 'forecast': 174,\n",
              " 'says': 175,\n",
              " 'might': 176,\n",
              " 'rain': 177,\n",
              " 'tomorrow': 178,\n",
              " \"don't\": 179,\n",
              " 'forget': 180,\n",
              " 'umbrella': 181,\n",
              " 'finished': 182,\n",
              " 'most': 183,\n",
              " 'book': 184,\n",
              " 'definitely': 185,\n",
              " 'check': 186,\n",
              " 'tried': 187,\n",
              " 'italian': 188,\n",
              " 'main': 189,\n",
              " 'pasta': 190,\n",
              " 'incredible': 191,\n",
              " 'excited': 192,\n",
              " 'summer': 193,\n",
              " 'vacation': 194,\n",
              " \"we're\": 195,\n",
              " 'planning': 196,\n",
              " 'visit': 197,\n",
              " 'europe': 198,\n",
              " 'feeling': 199,\n",
              " 'much': 200,\n",
              " 'better': 201,\n",
              " 'than': 202,\n",
              " 'saw': 203,\n",
              " 'apologize': 204,\n",
              " 'missing': 205,\n",
              " 'birthday': 206,\n",
              " 'party': 207,\n",
              " 'came': 208,\n",
              " 'favorite': 209,\n",
              " 'thing': 210,\n",
              " 'lazy': 211,\n",
              " 'sunday': 212,\n",
              " 'afternoon': 213,\n",
              " 'recommend': 214,\n",
              " 'mechanic': 215,\n",
              " 'car': 216,\n",
              " 'making': 217,\n",
              " 'strange': 218,\n",
              " 'noises': 219,\n",
              " 'looking': 220,\n",
              " 'forward': 221,\n",
              " 'exhausting': 222}"
            ]
          },
          "execution_count": 1629,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the word indes\n",
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1630,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVfLz4PogCOO",
        "outputId": "c199293e-0850-4907-9463-f8141578b1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['<NOTHING>', 'you', 'the', 'i', 'your', 'to', \"i'm\", 'this', 'how', 'have', 'a', 'new', 'it', 'for', 'that', 'really', 'did', 'was', 'just', 'been', 'up', 'weekend', 'today', 'morning', 'last', 'about', 'what', 'so', \"what's\", 'could', 'is', 'do', 'week', 'has', 'on', 'work', 'good', 'thinking', 'day', 'doing', 'had', 'weather', 'we', 'time', 'believe', 'wanted', 'me', 'amazing', 'late', 'like', 'something', 'street', 'nice', 'would', 'restaurant', \"how's\", 'at', 'should', 'are', 'year', 'love', 'out', 'my', 'look', 'reading', 'sleep', 'night', 'any', 'interesting', 'dreams', 'hey', 'there', 'other', 'beautiful', 'outside', 'chance', 'enjoy', 'yet', 'glad', 'finally', 'some', 'catch', 'in', 'life', 'crowded', 'supermarket', 'insane', 'say', 'thank', 'again', 'helping', 'move', \"you're\", 'end', 'going', 'concert', 'mentioned', 'sorry', 'running', 'traffic', 'absolutely', 'terrible', 'feel', 'having', 'lunch', 'craving', 'spicy', 'myself', 'seen', 'coffee', 'shop', 'opened', 'down', 'looks', \"can't\", 'fast', 'gone', 'by', 'friday', 'already', 'grab', 'dinner', 'sometime', 'know', 'great', 'family', 'heard', 'sister', 'baby', 'congratulations', 'tired', 'stayed', 'way', 'too', 'watching', 'show', 'netflix', 'hear', 'big', 'sale', 'happening', 'mall', 'go', 'plan', 'holidays', 'traveling', 'anywhere', 'exciting', 'outfit', 'color', 'brings', 'eyes', 'beautifully', 'help', 'with', 'computer', 'problem', 'completely', 'stuck', 'right', 'now', 'treating', 'lately', 'seem', 'busy', 'these', 'past', 'few', 'weeks', 'redecorating', 'living', 'room', 'think', 'forecast', 'says', 'might', 'rain', 'tomorrow', \"don't\", 'forget', 'umbrella', 'finished', 'most', 'book', 'definitely', 'check', 'tried', 'italian', 'main', 'pasta', 'incredible', 'excited', 'summer', 'vacation', \"we're\", 'planning', 'visit', 'europe', 'feeling', 'much', 'better', 'than', 'saw', 'apologize', 'missing', 'birthday', 'party', 'came', 'favorite', 'thing', 'lazy', 'sunday', 'afternoon', 'recommend', 'mechanic', 'car', 'making', 'strange', 'noises', 'looking', 'forward', 'exhausting'])"
            ]
          },
          "execution_count": 1630,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the words\n",
        "tokenizer.word_index.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1631,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amN1KgkvggiH",
        "outputId": "17ae957d-e243-4f5a-c9c1-d0ac89fb644f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_values([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222])"
            ]
          },
          "execution_count": 1631,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the values\n",
        "tokenizer.word_index.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1632,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg3qV7JlkKPH",
        "outputId": "2efa2a76-0726-4a5c-cf74-ebc3e56b39e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "execution_count": 1632,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.word_index.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1633,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svfldnylgi1O",
        "outputId": "506122b9-dee9-4294-b90f-8261eebf70fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[37, 24, 9, 17, 2, 66, 25, 67, 17, 2, 10, 68, 69, 70]\n",
            "[71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9, 10, 2, 20, 40]\n",
            "[27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6, 77, 3, 42, 78]\n",
            "[7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29, 12, 83, 5, 84]\n",
            "[30, 2, 45, 9, 85, 3, 86, 18, 8, 24, 13, 18, 87]\n",
            "[4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92, 25, 22, 93, 48]\n",
            "[9, 18, 5, 22, 17, 2, 94, 21, 95, 6, 15, 96, 2, 97]\n",
            "[7, 16, 98, 7, 99, 49, 100, 31, 101, 102, 8, 24]\n",
            "[27, 32, 2, 103, 50, 104, 14, 105, 23, 7, 106, 51, 107, 108]\n",
            "[10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52, 13, 114, 16, 53]\n",
            "[4, 115, 45, 9, 116, 8, 33, 34, 117, 118, 31, 13, 119, 120]\n",
            "[54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124, 11, 125, 12, 55]\n",
            "[56, 5, 126, 40, 4, 127, 5, 128, 19, 41, 11, 129, 130]\n",
            "[7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15, 12, 136, 35, 137]\n",
            "[17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8, 22, 43, 58, 143]\n",
            "[29, 5, 144, 14, 3, 145, 59, 2, 146, 147, 148, 8, 60]\n",
            "[4, 61, 5, 149, 23, 15, 150, 16, 151, 62, 5, 152, 153]\n",
            "[30, 2, 154, 47, 155, 8, 156, 157, 7, 158, 159, 160, 161]\n",
            "[56, 36, 20, 162, 2, 163, 2, 164, 16, 165, 166, 167, 168, 169]\n",
            "[7, 38, 26, 170, 63, 171, 172, 27, 32, 2, 173, 54, 64, 53]\n",
            "[3, 42, 174, 175, 13, 176, 177, 178, 179, 180, 5, 181]\n",
            "[4, 19, 182, 65, 3, 183, 48, 184, 2, 58, 185, 186, 13, 62]\n",
            "[10, 2, 187, 15, 12, 188, 55, 35, 189, 52, 3, 190, 31, 191]\n",
            "[7, 28, 192, 14, 193, 194, 195, 196, 6, 197, 198, 8, 60]\n",
            "[9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25, 44, 4, 203, 2]\n",
            "[4, 46, 6, 204, 14, 205, 5, 206, 207, 51, 208, 21, 57, 36]\n",
            "[29, 5, 209, 210, 6, 32, 35, 11, 211, 212, 213, 4, 61, 65]\n",
            "[30, 2, 214, 11, 37, 215, 63, 216, 34, 20, 217, 218, 219]\n",
            "[7, 16, 220, 221, 6, 3, 22, 8, 36, 33, 34, 20, 222]\n"
          ]
        }
      ],
      "source": [
        "# convert the sentences based on the text sequence\n",
        "for sentence in data.split(\"\\n\"):\n",
        "  if sentence.strip():\n",
        "    print(tokenizer.texts_to_sequences([sentence])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1634,
      "metadata": {
        "id": "GvTkEIJWhFbf"
      },
      "outputs": [],
      "source": [
        "# making the input and the output sequence\n",
        "input_sequences = []\n",
        "for sentence in data.split('\\n'):\n",
        "  if sentence.strip():\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0] # convert the sentences based on the text sequence\n",
        "\n",
        "    for i in range(1,len(tokenized_sentence)):\n",
        "      input_sequences.append(tokenized_sentence[:i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1635,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4B1mrA_-iU7I",
        "outputId": "f251d8e2-0bc8-4854-9f67-7ae9ca153726"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nso if X = 36 then y = 23\\nif X = 36,23 then y = 8\\n'"
            ]
          },
          "execution_count": 1635,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for 36 the next word will be 23\n",
        "# for 23 and 36 the next word will be 8\n",
        "\n",
        "\"\"\"\n",
        "so if X = 36 then y = 23\n",
        "if X = 36,23 then y = 8\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1636,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOi9rLLSiR4X",
        "outputId": "ea5a4c50-23f1-46aa-c965-6f00d83d3f4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[37, 24],\n",
              " [37, 24, 9],\n",
              " [37, 24, 9, 17],\n",
              " [37, 24, 9, 17, 2],\n",
              " [37, 24, 9, 17, 2, 66],\n",
              " [37, 24, 9, 17, 2, 66, 25],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17, 2],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17, 2, 10],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17, 2, 10, 68],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17, 2, 10, 68, 69],\n",
              " [37, 24, 9, 17, 2, 66, 25, 67, 17, 2, 10, 68, 69, 70],\n",
              " [71, 72],\n",
              " [71, 72, 4],\n",
              " [71, 72, 4, 18],\n",
              " [71, 72, 4, 18, 19],\n",
              " [71, 72, 4, 18, 19, 38],\n",
              " [71, 72, 4, 18, 19, 38, 26],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9, 10],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9, 10, 2],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9, 10, 2, 20],\n",
              " [71, 72, 4, 18, 19, 38, 26, 2, 3, 73, 39, 9, 10, 2, 20, 40],\n",
              " [27, 11],\n",
              " [27, 11, 74],\n",
              " [27, 11, 74, 39],\n",
              " [27, 11, 74, 39, 75],\n",
              " [27, 11, 74, 39, 75, 10],\n",
              " [27, 11, 74, 39, 75, 10, 2],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6, 77],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6, 77, 3],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6, 77, 3, 42],\n",
              " [27, 11, 74, 39, 75, 10, 2, 41, 11, 76, 6, 77, 3, 42, 78],\n",
              " [7, 28],\n",
              " [7, 28, 79],\n",
              " [7, 28, 79, 43],\n",
              " [7, 28, 79, 43, 80],\n",
              " [7, 28, 79, 43, 80, 10],\n",
              " [7, 28, 79, 43, 80, 10, 81],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29, 12],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29, 12, 83],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29, 12, 83, 5],\n",
              " [7, 28, 79, 43, 80, 10, 81, 44, 6, 82, 21, 29, 12, 83, 5, 84],\n",
              " [30, 2],\n",
              " [30, 2, 45],\n",
              " [30, 2, 45, 9],\n",
              " [30, 2, 45, 9, 85],\n",
              " [30, 2, 45, 9, 85, 3],\n",
              " [30, 2, 45, 9, 85, 3, 86],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18, 8],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18, 8, 24],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18, 8, 24, 13],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18, 8, 24, 13, 18],\n",
              " [30, 2, 45, 9, 85, 3, 86, 18, 8, 24, 13, 18, 87],\n",
              " [4, 19],\n",
              " [4, 19, 46],\n",
              " [4, 19, 46, 6],\n",
              " [4, 19, 46, 6, 88],\n",
              " [4, 19, 46, 6, 88, 89],\n",
              " [4, 19, 46, 6, 88, 89, 2],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92, 25],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92, 25, 22],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92, 25, 22, 93],\n",
              " [4, 19, 46, 6, 88, 89, 2, 90, 14, 91, 47, 92, 25, 22, 93, 48],\n",
              " [9, 18],\n",
              " [9, 18, 5],\n",
              " [9, 18, 5, 22],\n",
              " [9, 18, 5, 22, 17],\n",
              " [9, 18, 5, 22, 17, 2],\n",
              " [9, 18, 5, 22, 17, 2, 94],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95, 6],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95, 6, 15],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95, 6, 15, 96],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95, 6, 15, 96, 2],\n",
              " [9, 18, 5, 22, 17, 2, 94, 21, 95, 6, 15, 96, 2, 97],\n",
              " [7, 16],\n",
              " [7, 16, 98],\n",
              " [7, 16, 98, 7],\n",
              " [7, 16, 98, 7, 99],\n",
              " [7, 16, 98, 7, 99, 49],\n",
              " [7, 16, 98, 7, 99, 49, 100],\n",
              " [7, 16, 98, 7, 99, 49, 100, 31],\n",
              " [7, 16, 98, 7, 99, 49, 100, 31, 101],\n",
              " [7, 16, 98, 7, 99, 49, 100, 31, 101, 102],\n",
              " [7, 16, 98, 7, 99, 49, 100, 31, 101, 102, 8],\n",
              " [7, 16, 98, 7, 99, 49, 100, 31, 101, 102, 8, 24],\n",
              " [27, 32],\n",
              " [27, 32, 2],\n",
              " [27, 32, 2, 103],\n",
              " [27, 32, 2, 103, 50],\n",
              " [27, 32, 2, 103, 50, 104],\n",
              " [27, 32, 2, 103, 50, 104, 14],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23, 7],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23, 7, 106],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23, 7, 106, 51],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23, 7, 106, 51, 107],\n",
              " [27, 32, 2, 103, 50, 104, 14, 105, 23, 7, 106, 51, 107, 108],\n",
              " [10, 2],\n",
              " [10, 2, 109],\n",
              " [10, 2, 109, 3],\n",
              " [10, 2, 109, 3, 12],\n",
              " [10, 2, 109, 3, 12, 110],\n",
              " [10, 2, 109, 3, 12, 110, 111],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52, 13],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52, 13, 114],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52, 13, 114, 16],\n",
              " [10, 2, 109, 3, 12, 110, 111, 15, 112, 113, 3, 52, 13, 114, 16, 53],\n",
              " [4, 115],\n",
              " [4, 115, 45],\n",
              " [4, 115, 45, 9],\n",
              " [4, 115, 45, 9, 116],\n",
              " [4, 115, 45, 9, 116, 8],\n",
              " [4, 115, 45, 9, 116, 8, 33],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117, 118],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117, 118, 31],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117, 118, 31, 13],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117, 118, 31, 13, 119],\n",
              " [4, 115, 45, 9, 116, 8, 33, 34, 117, 118, 31, 13, 119, 120],\n",
              " [54, 2],\n",
              " [54, 2, 50],\n",
              " [54, 2, 50, 6],\n",
              " [54, 2, 50, 6, 121],\n",
              " [54, 2, 50, 6, 121, 122],\n",
              " [54, 2, 50, 6, 121, 122, 123],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124, 11],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124, 11, 125],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124, 11, 125, 12],\n",
              " [54, 2, 50, 6, 121, 122, 123, 8, 33, 4, 124, 11, 125, 12, 55],\n",
              " [56, 5],\n",
              " [56, 5, 126],\n",
              " [56, 5, 126, 40],\n",
              " [56, 5, 126, 40, 4],\n",
              " [56, 5, 126, 40, 4, 127],\n",
              " [56, 5, 126, 40, 4, 127, 5],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128, 19],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128, 19, 41],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128, 19, 41, 11],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128, 19, 41, 11, 129],\n",
              " [56, 5, 126, 40, 4, 127, 5, 128, 19, 41, 11, 129, 130],\n",
              " [7, 28],\n",
              " [7, 28, 131],\n",
              " [7, 28, 131, 23],\n",
              " [7, 28, 131, 23, 4],\n",
              " [7, 28, 131, 23, 4, 132],\n",
              " [7, 28, 131, 23, 4, 132, 21],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15, 12],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15, 12, 136],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15, 12, 136, 35],\n",
              " [7, 28, 131, 23, 4, 132, 21, 133, 134, 49, 135, 15, 12, 136, 35, 137],\n",
              " [17, 2],\n",
              " [17, 2, 138],\n",
              " [17, 2, 138, 26],\n",
              " [17, 2, 138, 26, 3],\n",
              " [17, 2, 138, 26, 3, 139],\n",
              " [17, 2, 138, 26, 3, 139, 140],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8, 22],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8, 22, 43],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8, 22, 43, 58],\n",
              " [17, 2, 138, 26, 3, 139, 140, 141, 57, 3, 142, 8, 22, 43, 58, 143],\n",
              " [29, 5],\n",
              " [29, 5, 144],\n",
              " [29, 5, 144, 14],\n",
              " [29, 5, 144, 14, 3],\n",
              " [29, 5, 144, 14, 3, 145],\n",
              " [29, 5, 144, 14, 3, 145, 59],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2, 146],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2, 146, 147],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2, 146, 147, 148],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2, 146, 147, 148, 8],\n",
              " [29, 5, 144, 14, 3, 145, 59, 2, 146, 147, 148, 8, 60],\n",
              " [4, 61],\n",
              " [4, 61, 5],\n",
              " [4, 61, 5, 149],\n",
              " [4, 61, 5, 149, 23],\n",
              " [4, 61, 5, 149, 23, 15],\n",
              " [4, 61, 5, 149, 23, 15, 150],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16, 151],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16, 151, 62],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16, 151, 62, 5],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16, 151, 62, 5, 152],\n",
              " [4, 61, 5, 149, 23, 15, 150, 16, 151, 62, 5, 152, 153],\n",
              " [30, 2],\n",
              " [30, 2, 154],\n",
              " [30, 2, 154, 47],\n",
              " [30, 2, 154, 47, 155],\n",
              " [30, 2, 154, 47, 155, 8],\n",
              " [30, 2, 154, 47, 155, 8, 156],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157, 7],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157, 7, 158],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157, 7, 158, 159],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157, 7, 158, 159, 160],\n",
              " [30, 2, 154, 47, 155, 8, 156, 157, 7, 158, 159, 160, 161],\n",
              " [56, 36],\n",
              " [56, 36, 20],\n",
              " [56, 36, 20, 162],\n",
              " [56, 36, 20, 162, 2],\n",
              " [56, 36, 20, 162, 2, 163],\n",
              " [56, 36, 20, 162, 2, 163, 2],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16, 165],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16, 165, 166],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16, 165, 166, 167],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16, 165, 166, 167, 168],\n",
              " [56, 36, 20, 162, 2, 163, 2, 164, 16, 165, 166, 167, 168, 169],\n",
              " [7, 38],\n",
              " [7, 38, 26],\n",
              " [7, 38, 26, 170],\n",
              " [7, 38, 26, 170, 63],\n",
              " [7, 38, 26, 170, 63, 171],\n",
              " [7, 38, 26, 170, 63, 171, 172],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32, 2],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32, 2, 173],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32, 2, 173, 54],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32, 2, 173, 54, 64],\n",
              " [7, 38, 26, 170, 63, 171, 172, 27, 32, 2, 173, 54, 64, 53],\n",
              " [3, 42],\n",
              " [3, 42, 174],\n",
              " [3, 42, 174, 175],\n",
              " [3, 42, 174, 175, 13],\n",
              " [3, 42, 174, 175, 13, 176],\n",
              " [3, 42, 174, 175, 13, 176, 177],\n",
              " [3, 42, 174, 175, 13, 176, 177, 178],\n",
              " [3, 42, 174, 175, 13, 176, 177, 178, 179],\n",
              " [3, 42, 174, 175, 13, 176, 177, 178, 179, 180],\n",
              " [3, 42, 174, 175, 13, 176, 177, 178, 179, 180, 5],\n",
              " [3, 42, 174, 175, 13, 176, 177, 178, 179, 180, 5, 181],\n",
              " [4, 19],\n",
              " [4, 19, 182],\n",
              " [4, 19, 182, 65],\n",
              " [4, 19, 182, 65, 3],\n",
              " [4, 19, 182, 65, 3, 183],\n",
              " [4, 19, 182, 65, 3, 183, 48],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2, 58],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2, 58, 185],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2, 58, 185, 186],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2, 58, 185, 186, 13],\n",
              " [4, 19, 182, 65, 3, 183, 48, 184, 2, 58, 185, 186, 13, 62],\n",
              " [10, 2],\n",
              " [10, 2, 187],\n",
              " [10, 2, 187, 15],\n",
              " [10, 2, 187, 15, 12],\n",
              " [10, 2, 187, 15, 12, 188],\n",
              " [10, 2, 187, 15, 12, 188, 55],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189, 52],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189, 52, 3],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189, 52, 3, 190],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189, 52, 3, 190, 31],\n",
              " [10, 2, 187, 15, 12, 188, 55, 35, 189, 52, 3, 190, 31, 191],\n",
              " [7, 28],\n",
              " [7, 28, 192],\n",
              " [7, 28, 192, 14],\n",
              " [7, 28, 192, 14, 193],\n",
              " [7, 28, 192, 14, 193, 194],\n",
              " [7, 28, 192, 14, 193, 194, 195],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196, 6],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196, 6, 197],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196, 6, 197, 198],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196, 6, 197, 198, 8],\n",
              " [7, 28, 192, 14, 193, 194, 195, 196, 6, 197, 198, 8, 60],\n",
              " [9, 59],\n",
              " [9, 59, 2],\n",
              " [9, 59, 2, 199],\n",
              " [9, 59, 2, 199, 23],\n",
              " [9, 59, 2, 199, 23, 2],\n",
              " [9, 59, 2, 199, 23, 2, 64],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25, 44],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25, 44, 4],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25, 44, 4, 203],\n",
              " [9, 59, 2, 199, 23, 2, 64, 200, 201, 202, 3, 25, 44, 4, 203, 2],\n",
              " [4, 46],\n",
              " [4, 46, 6],\n",
              " [4, 46, 6, 204],\n",
              " [4, 46, 6, 204, 14],\n",
              " [4, 46, 6, 204, 14, 205],\n",
              " [4, 46, 6, 204, 14, 205, 5],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207, 51],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207, 51, 208],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207, 51, 208, 21],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207, 51, 208, 21, 57],\n",
              " [4, 46, 6, 204, 14, 205, 5, 206, 207, 51, 208, 21, 57, 36],\n",
              " [29, 5],\n",
              " [29, 5, 209],\n",
              " [29, 5, 209, 210],\n",
              " [29, 5, 209, 210, 6],\n",
              " [29, 5, 209, 210, 6, 32],\n",
              " [29, 5, 209, 210, 6, 32, 35],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211, 212],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211, 212, 213],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211, 212, 213, 4],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211, 212, 213, 4, 61],\n",
              " [29, 5, 209, 210, 6, 32, 35, 11, 211, 212, 213, 4, 61, 65],\n",
              " [30, 2],\n",
              " [30, 2, 214],\n",
              " [30, 2, 214, 11],\n",
              " [30, 2, 214, 11, 37],\n",
              " [30, 2, 214, 11, 37, 215],\n",
              " [30, 2, 214, 11, 37, 215, 63],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216, 34],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216, 34, 20],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216, 34, 20, 217],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216, 34, 20, 217, 218],\n",
              " [30, 2, 214, 11, 37, 215, 63, 216, 34, 20, 217, 218, 219],\n",
              " [7, 16],\n",
              " [7, 16, 220],\n",
              " [7, 16, 220, 221],\n",
              " [7, 16, 220, 221, 6],\n",
              " [7, 16, 220, 221, 6, 3],\n",
              " [7, 16, 220, 221, 6, 3, 22],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8, 36],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8, 36, 33],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8, 36, 33, 34],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8, 36, 33, 34, 20],\n",
              " [7, 16, 220, 221, 6, 3, 22, 8, 36, 33, 34, 20, 222]]"
            ]
          },
          "execution_count": 1636,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1637,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIjtdOcOiw5f",
        "outputId": "d45140dc-07d9-4d3a-ea0b-761e40b7b700"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 1637,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the maximum length\n",
        "maximum_len = max([len(x) for x in input_sequences])\n",
        "maximum_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1638,
      "metadata": {
        "id": "xbAoCv00iw2X"
      },
      "outputs": [],
      "source": [
        "# padding (pre padding)\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = maximum_len, padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1639,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQFbSHEciw0G",
        "outputId": "dab61acc-bed1-4495-8a47-7f0c2857cd07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  37,  24],\n",
              "       [  0,   0,   0, ...,  37,  24,   9],\n",
              "       [  0,   0,   0, ...,  24,   9,  17],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  36,  33,  34],\n",
              "       [  0,   0,   0, ...,  33,  34,  20],\n",
              "       [  0,   0,   0, ...,  34,  20, 222]], dtype=int32)"
            ]
          },
          "execution_count": 1639,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1640,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2CWNeoJiwwX",
        "outputId": "f272c85b-5598-4fa1-9245-05d93d47e7b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(381, 16)"
            ]
          },
          "execution_count": 1640,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_input_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1641,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TtGo25xiwsI",
        "outputId": "38b09637-f15a-4bc5-d100-e1016261a4a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  0, 37],\n",
              "       [ 0,  0,  0, ...,  0, 37, 24],\n",
              "       [ 0,  0,  0, ..., 37, 24,  9],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ...,  8, 36, 33],\n",
              "       [ 0,  0,  0, ..., 36, 33, 34],\n",
              "       [ 0,  0,  0, ..., 33, 34, 20]], dtype=int32)"
            ]
          },
          "execution_count": 1641,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1642,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c6DaSq8j0L2",
        "outputId": "fe76804f-7089-41b3-fe32-25634e55d11b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(381, 15)"
            ]
          },
          "execution_count": 1642,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1643,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxZQv2UGiwpb",
        "outputId": "23fad85b-1022-4a4f-fa62-c556097ac24b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 24,   9,  17,   2,  66,  25,  67,  17,   2,  10,  68,  69,  70,\n",
              "        72,   4,  18,  19,  38,  26,   2,   3,  73,  39,   9,  10,   2,\n",
              "        20,  40,  11,  74,  39,  75,  10,   2,  41,  11,  76,   6,  77,\n",
              "         3,  42,  78,  28,  79,  43,  80,  10,  81,  44,   6,  82,  21,\n",
              "        29,  12,  83,   5,  84,   2,  45,   9,  85,   3,  86,  18,   8,\n",
              "        24,  13,  18,  87,  19,  46,   6,  88,  89,   2,  90,  14,  91,\n",
              "        47,  92,  25,  22,  93,  48,  18,   5,  22,  17,   2,  94,  21,\n",
              "        95,   6,  15,  96,   2,  97,  16,  98,   7,  99,  49, 100,  31,\n",
              "       101, 102,   8,  24,  32,   2, 103,  50, 104,  14, 105,  23,   7,\n",
              "       106,  51, 107, 108,   2, 109,   3,  12, 110, 111,  15, 112, 113,\n",
              "         3,  52,  13, 114,  16,  53, 115,  45,   9, 116,   8,  33,  34,\n",
              "       117, 118,  31,  13, 119, 120,   2,  50,   6, 121, 122, 123,   8,\n",
              "        33,   4, 124,  11, 125,  12,  55,   5, 126,  40,   4, 127,   5,\n",
              "       128,  19,  41,  11, 129, 130,  28, 131,  23,   4, 132,  21, 133,\n",
              "       134,  49, 135,  15,  12, 136,  35, 137,   2, 138,  26,   3, 139,\n",
              "       140, 141,  57,   3, 142,   8,  22,  43,  58, 143,   5, 144,  14,\n",
              "         3, 145,  59,   2, 146, 147, 148,   8,  60,  61,   5, 149,  23,\n",
              "        15, 150,  16, 151,  62,   5, 152, 153,   2, 154,  47, 155,   8,\n",
              "       156, 157,   7, 158, 159, 160, 161,  36,  20, 162,   2, 163,   2,\n",
              "       164,  16, 165, 166, 167, 168, 169,  38,  26, 170,  63, 171, 172,\n",
              "        27,  32,   2, 173,  54,  64,  53,  42, 174, 175,  13, 176, 177,\n",
              "       178, 179, 180,   5, 181,  19, 182,  65,   3, 183,  48, 184,   2,\n",
              "        58, 185, 186,  13,  62,   2, 187,  15,  12, 188,  55,  35, 189,\n",
              "        52,   3, 190,  31, 191,  28, 192,  14, 193, 194, 195, 196,   6,\n",
              "       197, 198,   8,  60,  59,   2, 199,  23,   2,  64, 200, 201, 202,\n",
              "         3,  25,  44,   4, 203,   2,  46,   6, 204,  14, 205,   5, 206,\n",
              "       207,  51, 208,  21,  57,  36,   5, 209, 210,   6,  32,  35,  11,\n",
              "       211, 212, 213,   4,  61,  65,   2, 214,  11,  37, 215,  63, 216,\n",
              "        34,  20, 217, 218, 219,  16, 220, 221,   6,   3,  22,   8,  36,\n",
              "        33,  34,  20, 222], dtype=int32)"
            ]
          },
          "execution_count": 1643,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = padded_input_sequences[:,-1]\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1644,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehDrOC6vjyRI",
        "outputId": "0bf00610-97e1-4030-e9d7-f5a7c80edf95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(381,)"
            ]
          },
          "execution_count": 1644,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1645,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu2jsydhpZzI",
        "outputId": "ebb05d26-3c58-4d71-d852-a4c5f4461435"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "223"
            ]
          },
          "execution_count": 1645,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) +1 # +1 fo including the zero index\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1646,
      "metadata": {
        "id": "rWhaAC4Xj8cw"
      },
      "outputs": [],
      "source": [
        "# applying One-Hot-Encoding on the traget column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1647,
      "metadata": {
        "id": "wnbc0HJQiwnG"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(y,num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1648,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYe6jmOviwkR",
        "outputId": "f0f762d8-680e-4e78-eb5c-35f3782f8422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(381, 223)"
            ]
          },
          "execution_count": 1648,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1649,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmgP50d0muMo",
        "outputId": "32881d52-6fca-49e2-e194-863a7f8427ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "223\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer.word_index) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1650,
      "metadata": {
        "id": "3R4qAkAwiwcm"
      },
      "outputs": [],
      "source": [
        "# makin the RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1651,
      "metadata": {
        "id": "JLBlGm6siwXv"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1652,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjLm37GEymwg",
        "outputId": "40c47ef4-b29a-4863-bd5c-e72125222c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the vocab size or the total no. word size is 223\n",
            "the maximun length sentence in the data is 16\n"
          ]
        }
      ],
      "source": [
        "print(f\"the vocab size or the total no. word size is {vocab_size}\")\n",
        "print(f\"the maximun length sentence in the data is {maximum_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "FOR Embedding layer:\n",
        "    input_dim:\n",
        "    - Meaning: This is the size of the vocabulary, i.e., the number of unique words or tokens in your dataset\n",
        "\n",
        "    output_dim:\n",
        "    - Meaning: This specifies the dimensionality of the output embedding vectors for each word.\n",
        "\n",
        "    input_length:\n",
        "    - Meaning: This specifies the length of the input sequences to the Embedding layer (i.e., the max number of words per sequence).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1653,
      "metadata": {
        "id": "XLjR2vajiwVD"
      },
      "outputs": [],
      "source": [
        "# embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=50,input_shape=(maximum_len-1,))) # output_dim is the output vector\n",
        "# first RNN layer\n",
        "model.add(SimpleRNN(20,activation='relu',return_sequences=True)) # by writing input_sequences=True it will connect wit the next RNN layer\n",
        "# second RNN layer\n",
        "model.add(SimpleRNN(20,activation='relu'))\n",
        "# output layer\n",
        "model.add(Dense(vocab_size,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1654,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "X0zG2BBAliMQ",
        "outputId": "356d9234-a9cb-4758-e41b-d0139bf8d3eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_46\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_46\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " embedding_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">11,150</span> \n",
              "\n",
              " simple_rnn_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,420</span> \n",
              "\n",
              " simple_rnn_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">820</span> \n",
              "\n",
              " dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">223</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,683</span> \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " embedding_48 (\u001b[38;5;33mEmbedding\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m50\u001b[0m)                 \u001b[38;5;34m11,150\u001b[0m \n",
              "\n",
              " simple_rnn_51 (\u001b[38;5;33mSimpleRNN\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  \u001b[38;5;34m1,420\u001b[0m \n",
              "\n",
              " simple_rnn_52 (\u001b[38;5;33mSimpleRNN\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                        \u001b[38;5;34m820\u001b[0m \n",
              "\n",
              " dense_45 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m223\u001b[0m)                     \u001b[38;5;34m4,683\u001b[0m \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,073</span> (70.60 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,073\u001b[0m (70.60 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,073</span> (70.60 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,073\u001b[0m (70.60 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1655,
      "metadata": {
        "id": "az_uQXZ7lmg_"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1656,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcoJt-k_luZI",
        "outputId": "de0992f6-fe6f-4786-c202-219631721e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.0023 - loss: 5.4069   \n",
            "Epoch 2/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0221 - loss: 5.3942\n",
            "Epoch 3/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0222 - loss: 5.3695\n",
            "Epoch 4/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0215 - loss: 5.3089\n",
            "Epoch 5/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0841 - loss: 5.1542\n",
            "Epoch 6/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0480 - loss: 5.1164\n",
            "Epoch 7/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0523 - loss: 5.0508\n",
            "Epoch 8/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0635 - loss: 4.9293\n",
            "Epoch 9/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0786 - loss: 4.7379\n",
            "Epoch 10/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0956 - loss: 4.7529\n",
            "Epoch 11/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0767 - loss: 4.6877\n",
            "Epoch 12/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0868 - loss: 4.5577\n",
            "Epoch 13/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0934 - loss: 4.3702\n",
            "Epoch 14/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0853 - loss: 4.3515\n",
            "Epoch 15/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1112 - loss: 4.1751\n",
            "Epoch 16/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1200 - loss: 4.0720\n",
            "Epoch 17/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1039 - loss: 3.9577\n",
            "Epoch 18/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1598 - loss: 3.8129\n",
            "Epoch 19/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1760 - loss: 3.5590\n",
            "Epoch 20/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1957 - loss: 3.3978\n",
            "Epoch 21/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2563 - loss: 3.2181\n",
            "Epoch 22/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2309 - loss: 3.0574\n",
            "Epoch 23/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3354 - loss: 2.8561\n",
            "Epoch 24/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4019 - loss: 2.6334\n",
            "Epoch 25/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4442 - loss: 2.3232\n",
            "Epoch 26/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4329 - loss: 2.3378\n",
            "Epoch 27/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4504 - loss: 2.1389\n",
            "Epoch 28/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5476 - loss: 1.9153\n",
            "Epoch 29/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5506 - loss: 1.9033\n",
            "Epoch 30/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5435 - loss: 1.7719\n",
            "Epoch 31/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5998 - loss: 1.5908\n",
            "Epoch 32/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6478 - loss: 1.5042\n",
            "Epoch 33/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6867 - loss: 1.3641\n",
            "Epoch 34/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6921 - loss: 1.3389\n",
            "Epoch 35/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6929 - loss: 1.3512\n",
            "Epoch 36/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7262 - loss: 1.1501\n",
            "Epoch 37/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7141 - loss: 1.2196\n",
            "Epoch 38/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7754 - loss: 1.0193\n",
            "Epoch 39/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7346 - loss: 1.0408\n",
            "Epoch 40/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7771 - loss: 0.9534\n",
            "Epoch 41/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7465 - loss: 0.9984\n",
            "Epoch 42/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7828 - loss: 0.9262\n",
            "Epoch 43/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8108 - loss: 0.8057\n",
            "Epoch 44/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7784 - loss: 0.8347\n",
            "Epoch 45/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7776 - loss: 0.8893\n",
            "Epoch 46/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7785 - loss: 0.8983\n",
            "Epoch 47/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8032 - loss: 0.7968\n",
            "Epoch 48/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8278 - loss: 0.7549\n",
            "Epoch 49/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8229 - loss: 0.6943\n",
            "Epoch 50/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8059 - loss: 0.7647\n",
            "Epoch 51/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8092 - loss: 0.7866\n",
            "Epoch 52/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8282 - loss: 0.6582\n",
            "Epoch 53/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8274 - loss: 0.7092\n",
            "Epoch 54/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8792 - loss: 0.5439\n",
            "Epoch 55/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8725 - loss: 0.5866\n",
            "Epoch 56/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8362 - loss: 0.6038\n",
            "Epoch 57/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8489 - loss: 0.5788\n",
            "Epoch 58/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8704 - loss: 0.5327\n",
            "Epoch 59/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8622 - loss: 0.5620\n",
            "Epoch 60/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8921 - loss: 0.4372\n",
            "Epoch 61/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8547 - loss: 0.5756\n",
            "Epoch 62/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8771 - loss: 0.5278\n",
            "Epoch 63/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8619 - loss: 0.5717\n",
            "Epoch 64/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8385 - loss: 0.5791\n",
            "Epoch 65/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8581 - loss: 0.5298\n",
            "Epoch 66/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8640 - loss: 0.5372\n",
            "Epoch 67/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8921 - loss: 0.4282\n",
            "Epoch 68/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8697 - loss: 0.4109\n",
            "Epoch 69/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8672 - loss: 0.4659\n",
            "Epoch 70/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8991 - loss: 0.4215\n",
            "Epoch 71/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8730 - loss: 0.4356\n",
            "Epoch 72/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8703 - loss: 0.4766\n",
            "Epoch 73/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8594 - loss: 0.4828\n",
            "Epoch 74/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8867 - loss: 0.4507\n",
            "Epoch 75/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9091 - loss: 0.3789\n",
            "Epoch 76/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9114 - loss: 0.3853\n",
            "Epoch 77/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8826 - loss: 0.4288\n",
            "Epoch 78/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8753 - loss: 0.4321\n",
            "Epoch 79/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8930 - loss: 0.3948\n",
            "Epoch 80/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9071 - loss: 0.3773\n",
            "Epoch 81/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8899 - loss: 0.3442\n",
            "Epoch 82/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8883 - loss: 0.4055\n",
            "Epoch 83/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9161 - loss: 0.3171\n",
            "Epoch 84/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8957 - loss: 0.3656\n",
            "Epoch 85/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9048 - loss: 0.3642\n",
            "Epoch 86/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9017 - loss: 0.4034\n",
            "Epoch 87/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9071 - loss: 0.3274\n",
            "Epoch 88/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9202 - loss: 0.2948\n",
            "Epoch 89/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9121 - loss: 0.3276\n",
            "Epoch 90/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9010 - loss: 0.3358\n",
            "Epoch 91/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9051 - loss: 0.3329\n",
            "Epoch 92/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8993 - loss: 0.3320\n",
            "Epoch 93/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9157 - loss: 0.2999\n",
            "Epoch 94/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9201 - loss: 0.3166\n",
            "Epoch 95/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9197 - loss: 0.2965\n",
            "Epoch 96/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9106 - loss: 0.2798\n",
            "Epoch 97/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9136 - loss: 0.3193\n",
            "Epoch 98/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9073 - loss: 0.3142\n",
            "Epoch 99/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8842 - loss: 0.3542\n",
            "Epoch 100/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9396 - loss: 0.2432\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79f47e1871d0>"
            ]
          },
          "execution_count": 1656,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X,y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1662,
      "metadata": {
        "id": "8l7y8sH7lwux"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def predict_next_word(user_text, num_words=8):\n",
        "    \"\"\"\n",
        "    Predicts the next words iteratively based on the given seed text and generates a sequence of words.\n",
        "\n",
        "    Arguments:\n",
        "    user_text -- the initial text (string) to start the prediction\n",
        "    num_words -- the number of words to predict iteratively (default is 10)\n",
        "\n",
        "    Returns:\n",
        "    A string containing the seed text followed by the predicted words.\n",
        "    \"\"\"\n",
        "    # Loop to predict 'num_words' words one by one\n",
        "    for i in range(num_words):\n",
        "        # Step 1: Tokenize the seed text into a sequence of integers\n",
        "        # This converts each word in the seed text into a corresponding integer based on the tokenizer's word index\n",
        "        token_text = tokenizer.texts_to_sequences([user_text])[0]\n",
        "\n",
        "        # Step 2: Pad the tokenized sequence to ensure it matches the model's expected input shape\n",
        "        # Padding ensures that all input sequences are the same length (max_len), and 'pre' means padding is added at the start\n",
        "        padded_token_text = pad_sequences([token_text], maxlen=maximum_len, padding='pre')\n",
        "\n",
        "        # Step 3: Use the model to predict the next word's probabilities\n",
        "        # The model outputs a probability distribution over the vocabulary for the next word\n",
        "        pred = model.predict(padded_token_text)\n",
        "\n",
        "        # Step 4: Get the index of the word with the highest predicted probability\n",
        "        # 'np.argmax' returns the index of the word with the highest probability\n",
        "        pred_word_index = np.argmax(pred)\n",
        "\n",
        "        # Step 5: Retrieve the predicted word from the tokenizer's word index\n",
        "        # We use the word index to map the predicted index back to the corresponding word\n",
        "        pred_word = tokenizer.index_word.get(pred_word_index, None)\n",
        "\n",
        "        # Step 6: Add the predicted word to the seed text\n",
        "        # If a valid word is predicted, append it to the final pred\n",
        "        if pred_word:\n",
        "            user_text = user_text + \" \" + pred_word\n",
        "            print(user_text)  # Print the updated seed text with the new predicted word\n",
        "\n",
        "        # Step 7: Sleep for 2 seconds before predicting the next word\n",
        "        # This introduces a slight delay between predictions to simulate more natural generation\n",
        "        time.sleep(2)\n",
        "\n",
        "    return user_text  # Return the final generated text containing the original seed text and the predicted words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1663,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWRnd3gdqt8A",
        "outputId": "c2f0745e-d370-4317-9564-576fe1428965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "What's your plan for\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "What's your plan for the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "What's your plan for the holidays\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "What's your plan for the holidays are\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "What's your plan for the holidays are you\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "What's your plan for the holidays are you traveling\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "What's your plan for the holidays are you traveling anywhere\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "What's your plan for the holidays are you traveling anywhere exciting\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"What's your plan\"\n",
        "generated_text = predict_next_word(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1658,
      "metadata": {
        "id": "yhaPvG9Mu7Tz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
